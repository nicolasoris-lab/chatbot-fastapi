from llm_guard.input_scanners import PromptInjection

print("üîÑ Cargando el modelo del esc√°ner...")
scanner = PromptInjection()
print("‚úÖ Modelo cargado. La funci√≥n est√° lista para usarse.")

def is_valid_prompt(user_input: str) -> bool:
    """
    Analiza el texto de un usuario con LLM Guard para detectar inyecci√≥n de prompts.

    Args:
        user_input: El string de entrada proporcionado por el usuario.

    Returns:
        True si el prompt es considerado seguro (v√°lido).
        False si se detecta un posible ataque de inyecci√≥n (inv√°lido).
    """
    _, is_valid = scanner.scan(user_input)
    return is_valid